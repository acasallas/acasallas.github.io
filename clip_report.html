<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Alan Casallas</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/clean-blog.min.css" rel="stylesheet">
    
    <!-- Custom CSS overrides -->
    <link href="css/mycss.css" rel="stylesheet">

  </head>

  <body>

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand" href="index.html">Main Page</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fa fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="projects.html">Projects</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="alan_casallas_resume.pdf">Resume</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <style>
  .projectimg {
    display: inline;
    float:left;
    padding:10px;
    border-radius:10%;
  }
</style>

    <!-- Page Header -->
    <header class="masthead" style="background-color: white">
      <div class="overlay"></div>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <div class="page-heading">
              <h1>CLIP Report</h1>
            </div>
          </div>
        </div>
      </div>
    </header>

    <div class="col-lg-8 col-md-10 mx-auto">
    <p>
      Please see the <a href="https://github.com/acasallas/clip-ablation-study">GitHub Repo.</a>
    </p>

    <h3>Motivation</h3>
    <p>
      I created a custom implementation of CLIP. I did both to explore what effect replacing a transformer with an RNN would have as the text encoder, as well as to explore how rich embeddings trained on a home GPU (Nvidia 4090) could be. 
  </p>
  <p>The training set I chose was pixparse/cc3m-wds. I chose this training set because its text descriptions are in English, in fairly simple and descriptive sentences.
    It was a much cleaner dataset than others I analyzed, such as mlfoundations/datacomp, whose text was in many different languages and often not fully descriptive of the associated image.
  </p>
  <img src="img/cc3m_dataset.png" height="300">
  <p>To keep my parameter count down, I downsampled and cropped images to 128x128 pixels, but kept color channels.</p>
  <p>When OpenAI trained CLIP, their smallest model was trained with a batch size of 32,768. In many deep learning problems, the loss function is an average of each individual sample's loss with respect to a true answer, and so to achieve a large effective batch size, it is possible to calculate loss for a smaller "micro batch" of samples and simply sum the results to calculate the loss for a larger effective batch. In CLIP however, the loss is not an average of individual samples, but rather is contrastive in nature and is thus a comparison of each pair of samples, making it less obvious how to calculate the loss for a large effective batch.</p>
  <img src="img/clip_loss.png" height="100">
  <h3>Training</h3>
  <p>With my model and data setup, I was able to fit a 2048 sample batch size in my GPU. To try to achieve a larger effective batch size, I implemented an XBM queue of size 2048, saving the most recent embeddings in the queue so they could be appended to the 256 items in the current batch for loss calculation.</p>
  <p>Intially when I began training with this setup, the network failed to learn, even after trying a variety of learning rates. The network only started to learn after I removed the XBM queue. It appears that having an XBM queue made it difficult for my network to learn, either due to the sheer size of the embeddings, or, more likely, because the network was not responding well to having stale embeddings in its loss function. Once I removed the XBM queue, the network started to learn, as can be seen from the wandb plots below.</p> 
  
  <img src="img/clip_training.png" height="400">
  <p>While all training from this point on was done without the XBM queue, a path for further experimentation could be to gradually introduce an increasingly large XBM queue after training loss has started to decrease or plateau. 
    Another alternative is to use a similar technique as MoCo, which keeps two networks, one that is trained, and another that is used to generate embeddings and is smoothed with an ema of parameters.</p>
    <h3>Results</h3>
  <p>As can be seen from the wandb results, the validation accuracy reached 13% and 16% for text and image embedding accuracy, respectivley. 
    Since a batch size of 2048 would have a random untrained accuracy of 0.04%, it is evident the model learned to associate image and text embeddings.
  To test the limits of the embeddings, I used them to try to classify the 1000-class Imagenet dataset using the same procedure OpeanAI did, that is, creating a sentence such as "this is a picture of {}" and inserting the imagenet class name in the sentence.
  Top-1 Imagenet accuracy was 8.3% and Top-5 accuracy was 19.6%. The somewhat disappointing performance is understandable because ImageNet labels are somewhat non-descriptive and esoteric (labels include 'agaric' and 'gyromitra', words I myself do not konw the meaning of).
To attempt to give the model a chance to improve performance, I had LLM's generate a simple worded one sentence description of each label (for example, the sentence for agaric became "agaric is a type of mushroom with gills." However, even with this prompt engineering, zero-shot performance on Imagenet remained low, at 6.7% and 18.8% top-1 and top-5 accuracy, respectively.</p>
<p>Nevertheless, I was happy to see the model learn embeddings well above random levels and attribute its less than state-of-art performance to the small model size, small training set size, and the experimental use of an RNN as the text encoder. If my goal was to improve performance using the same hardware setup, my first next steps would be to train on more data (a 12M version of cc3m is available) and to switch to a transformer as the text encoder.</p>

    <hr>

    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <ul class="list-inline text-center">
              <li class="list-inline-item">
                <a href="https://www.facebook.com/alan.casallas.7">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://github.com/acasallas">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://www.linkedin.com/in/alancasallas/">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                </span>
                </a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/clean-blog.min.js"></script>

  </body>

</html>
